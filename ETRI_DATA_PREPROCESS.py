# %%
import os, sys
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import glob
import json
from time import time
import datetime
import ast
import random
from Utils.preprocessing import preprocess_all_days
from tqdm.auto import tqdm
from Utils.preprocessing import preprocess_wHr, preprocess_mGps, preprocess_mWifi, preprocess_mUsage, preprocess_mBle
from Utils.util import print_dict_structure

# %% [markdown]
# # PARAMETER

# %%
FREQUENCY = 5  # minutes
INTERPOLATION = "linear"  # 'time' or 'linear'
DATA = 0
MASK = 1
MODAL_NAME = ['mLight','mACStatus', 'mActivity', 'mBle', 'mGps', 'wHr',
                  'mScreenStatus', 'mUsageStats', 'mWifi', 'wLight', 'wPedo']
MIN_RATIO = 0.1  # ìµœì†Œ ë°ì´í„° ì»¤ë²„ë¦¬ì§€ ë¹„ìœ¨

orginal_data_freq = {
    "mACStatus": 1,
    "mActivity": 5,
    "mBle": 10,
    "mGps": 1,
    "mLight": 10,
    "mScreenStatus": 1,
    "mUsageStats": 10,
    "mWifi": 10,
    "wHr": 1,
    "wLight": 1,
    "wPedo": 1,
}

# %% [markdown]
# # Data items
# * mACStatus
# * mActivity
# * mBle
# * mGps
# * mLight
# * mScreenStatus
# * mUsageStatus
# * wHr
# * wLight
# * wPedo
# 

# %%
SD = 42
random.seed(SD)
np.random.seed(SD)
os.environ['PYTHONHASHSEED'] = str(SD)

# %%
dataset_path = os.path.join("ETRI 2024","ch2025_data_items")
train_data_path = os.path.join("ETRI 2024","ch2025_metrics_train.csv")

# %%
print("challenge 2025 dataset " + "="*5)
for file_name in os.listdir(dataset_path):
    if file_name.endswith(".parquet"):
        print(file_name)
        
parquet_files = glob.glob(os.path.join(dataset_path, "*.parquet"))
print(f"\nTotal parquet files: {len(parquet_files)}")




# %% [markdown]
# # .paruet íŒŒì¼ ë¡œë“œ

# %%
# íŒŒì¼ ì´ë¦„ì„ í‚¤ë¡œ, DataFrameì„ ê°’ìœ¼ë¡œ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
lifelog_data = {}

# íŒŒì¼ë³„ë¡œ ì½ê¸°
for file_path in parquet_files:
    name = os.path.basename(file_path).replace('.parquet', '').replace('ch2025_', '')
    lifelog_data[name] = pd.read_parquet(file_path)
    print(f"âœ… Loaded: {name}, shape = {lifelog_data[name].shape}")

# %%
# ë”•ì…”ë„ˆë¦¬ì— ìˆëŠ” ëª¨ë“  í•­ëª©ì„ ë…ë¦½ì ì¸ ë³€ìˆ˜ë¡œ í• ë‹¹
for key, df in lifelog_data.items():
    globals()[f"{key}_df"] = df

# %%
metric_train_df = pd.read_csv(train_data_path)
print(f"âœ… Loaded: metric_train_df, shape = {metric_train_df.shape}")
print(metric_train_df.head())

# %%
sample_submission = pd.read_csv(os.path.join("ETRI 2024","ch2025_submission_sample.csv"))
sample_submission['lifelog_date'] = pd.to_datetime(sample_submission['lifelog_date'])
test_keys = set(zip(sample_submission['subject_id'], sample_submission['lifelog_date'].dt.date))
print(f"âœ… Loaded: sample_submission, shape = {sample_submission.shape}")

# %%
# âœ… ë¶„ë¦¬ í•¨ìˆ˜
def split_test_train(df, subject_col='subject_id', timestamp_col='timestamp'):
    df[timestamp_col] = pd.to_datetime(df[timestamp_col], errors='coerce')
    df = df.dropna(subset=[timestamp_col])
    df['date_only'] = df[timestamp_col].dt.date
    df['key'] = list(zip(df[subject_col], df['date_only']))

    test_df = df[df['key'].isin(test_keys)].drop(columns=['date_only', 'key'])
    train_df = df[~df['key'].isin(test_keys)].drop(columns=['date_only', 'key'])
    return test_df, train_df


# %%
# âœ… DataFrame ë³„ timestamp ì»¬ëŸ¼ ìˆ˜ë™ ì§€ì •
dataframes = {
    'mACStatus': (mACStatus_df, 'timestamp'),
    'mActivity': (mActivity_df, 'timestamp'),
    'mAmbience': (mAmbience_df, 'timestamp'),
    'mBle': (mBle_df, 'timestamp'),
    'mGps': (mGps_df, 'timestamp'),
    'mLight': (mLight_df, 'timestamp'),
    'mScreenStatus': (mScreenStatus_df, 'timestamp'),
    'mUsageStats': (mUsageStats_df, 'timestamp'),
    'mWifi': (mWifi_df, 'timestamp'),
    'wHr': (wHr_df, 'timestamp'),
    'wLight': (wLight_df, 'timestamp'),
    'wPedo': (wPedo_df, 'timestamp'),
}

# %% [markdown]
# # í•™ìŠµ ë°ì´í„° ë¶„ë¦¬

# %%
# âœ… ê²°ê³¼ ì €ì¥
for name, (df, ts_col) in dataframes.items():
    print(f"â³ {name} ë¶„ë¦¬ ì¤‘...")
    test_df, train_df = split_test_train(df.copy(), subject_col='subject_id', timestamp_col=ts_col)
    globals()[f"{name}_test"] = test_df
    globals()[f"{name}_train"] = train_df
    print(f"âœ… {name}_test â†’ {test_df.shape}, {name}_train â†’ {train_df.shape}")

# %% [markdown]
# # 5ë¶„ ë‹¨ìœ„ í‰ê·  T=492
# * ë§Œì•½ ê²°ì¸¡ì¹˜ê°€ 5ë¶„ì´ ë„˜ì–´ê°ˆ ê²½ìš° ë³´ê°„
# * ì›ë˜ lifelog_Data, Sleep_data(lisfelog_data+1) ë‘˜ ë‹¤ ìˆìœ¼ë‚˜ í˜„ì¬ëŠ” ì˜ˆì¸¡ì´ê¸°ì— lifelog_dataë§Œ ì‚¬ìš©

# %%
modality_names = ['mACStatus', 'mActivity', 'mBle', 'mGps', 'mLight',
                  'mScreenStatus', 'mUsageStats', 'mWifi', 'wHr', 'wLight', 'wPedo']
for name in modality_names:   # ['BLE', 'HR', 'ACC', ...]
    train_df:pd.DataFrame = globals()[f"{name}_train"]
    os.makedirs(f"ETRI 2024/train", exist_ok=True)
    print(f"{name} info")
    print(train_df.info())
    print(train_df.head(2))
    print("\n")
    train_df.head(50).to_csv(f"ETRI 2024/train/{name}_train_sample.csv", index=False)

# %%
processed_dict = {}

modality_handlers = {
    "wHr": preprocess_wHr,
    "mGps": preprocess_mGps,
    "mWifi": preprocess_mWifi,
    "mUsageStats": preprocess_mUsage,
    "mBle": preprocess_mBle,
}



modality_names = ['mLight','mACStatus', 'mActivity', 'mBle', 'mGps', 'wHr',
                  'mScreenStatus', 'mUsageStats', 'mWifi', 'wLight', 'wPedo']
for name in modality_names:   # ['wHr', 'mBle', 'mWifi', ...]
    train_df = globals()[f"{name}_train"]

    
    print(f"â³ {name} ì „ì²˜ë¦¬ ì¤‘...")
    if name in modality_handlers.keys():
        preprocess_func = modality_handlers[name]
        train_df = preprocess_func(train_df)

    if not os.path.exists(f"ETRI 2024/train/{name}_train_preprocess_input.csv"):
        train_df.to_csv(f"ETRI 2024/train/{name}_train_preprocess_input.csv", index=False)
    proc = preprocess_all_days(
        df=train_df,
        metric_df=metric_train_df,
        resample_freq=FREQUENCY,
        interpolation=INTERPOLATION,
        min_ratio=MIN_RATIO,
        mask=True
    )

    processed_dict[name] = proc

# %%
processed_dict['mLight']

# %%
final_dataset = {}

keys = processed_dict["mACStatus"].keys()   # ê³µí†µ key

for key in keys:
    final_dataset[key] = {}
    for name in modality_names:
        if key in processed_dict[name]:
            final_dataset[key][name] = processed_dict[name][key] # KEY: MODALITY -> (subid, date)ë¡œ ë³€í™˜


# %%
file_name = f"ETRI 2024/processed_{INTERPOLATION}_{FREQUENCY}min_{MIN_RATIO*100:.0f}%_MIN_MASK_dataset.pkl"
import pickle
with open(file_name, "wb") as f:
    pickle.dump(final_dataset, f)

# %%
from collections import defaultdict

modality_names = processed_dict.keys()

# ëª¨ë‹¬ë¦¬í‹°ë³„ missing count
missing_by_modality = defaultdict(int)

# ë‚ ì§œë³„ ëª¨ë‹¬ë¦¬í‹° ê°œìˆ˜
modalities_per_day = defaultdict(int)

# ë‚ ì§œ ë¦¬ìŠ¤íŠ¸
all_keys = list(final_dataset.keys())

for key in all_keys:
    day_modalities = final_dataset[key]
    
    count_present = 0
    for m in modality_names:
        if m in day_modalities and day_modalities[m] is not None:
            count_present += 1
        else:
            missing_by_modality[m] += 1

    modalities_per_day[key] = count_present

# âœ” ëª¨ë“  ëª¨ë‹¬ë¦¬í‹°ê°€ ìˆëŠ” ë‚ ì§œ
complete_days = [k for k, c in modalities_per_day.items() if c == len(modality_names)]

# âœ” í•˜ë‚˜ë¼ë„ ë¶€ì¡±í•œ ë‚ ì§œ
incomplete_days = [k for k, c in modalities_per_day.items() if c < len(modality_names)]

# âœ” coverage percent
coverage = {m: 1 - missing_by_modality[m] / len(all_keys) for m in modality_names}


# ------------------ ì¶œë ¥ ------------------

print("ğŸ“Œ ëª¨ë‹¬ë¦¬í‹°ë³„ Missing ê°œìˆ˜:")
for m in modality_names:
    print(f"  - {m}: {missing_by_modality[m]}ê°œ missing")

print("\nğŸ“Œ ë‚ ì§œë³„ ëª¨ë‹¬ë¦¬í‹° ê°œìˆ˜ (ì˜ˆ: 5ê°œ ìˆìœ¼ë©´ 5ê°œ)")
for k, v in list(modalities_per_day.items())[:10]:  # ì• 10ê°œë§Œ ë¯¸ë¦¬ë³´ê¸°
    print(f"{k}: {v}ê°œ")

print("\nğŸ“Œ ëª¨ë“  ëª¨ë‹¬ë¦¬í‹°ê°€ ìˆëŠ” ë‚ ì§œ ê°œìˆ˜:", len(complete_days))
print("ğŸ“Œ í•˜ë‚˜ë¼ë„ ë¹ ì§„ ë‚ ì§œ ê°œìˆ˜:", len(incomplete_days))

print("\nğŸ“Œ ëª¨ë‹¬ë¦¬í‹°ë³„ coverage ë¹„ìœ¨ (%):")
for m in modality_names:
    print(f"  - {m}: {coverage[m]*100:.2f}%")



# %%

print_dict_structure(final_dataset, max_value_length=10)  # ì• 10ê°œ í‚¤ë§Œ ì¶œë ¥


